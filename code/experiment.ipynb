{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"experiment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vl-zzXroRTar","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620451695413,"user_tz":420,"elapsed":47480,"user":{"displayName":"MLAB Team0","photoUrl":"","userId":"09750010280646594005"}},"outputId":"cbc50bd2-bc68-44a3-ff23-4ad21314869f"},"source":["# Stable Baselines only supports tensorflow 1.x for now\n","%tensorflow_version 1.x\n","!apt install swig cmake libopenmpi-dev zlib1g-dev\n","!pip install stable-baselines[mpi]==2.10.0 box2d box2d-kengz\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n","zlib1g-dev set to manually installed.\n","libopenmpi-dev is already the newest version (2.1.1-8).\n","cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  swig3.0\n","Suggested packages:\n","  swig-doc swig-examples swig3.0-examples swig3.0-doc\n","The following NEW packages will be installed:\n","  swig swig3.0\n","0 upgraded, 2 newly installed, 0 to remove and 34 not upgraded.\n","Need to get 1,100 kB of archives.\n","After this operation, 5,822 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n","Fetched 1,100 kB in 3s (438 kB/s)\n","Selecting previously unselected package swig3.0.\n","(Reading database ... 160706 files and directories currently installed.)\n","Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n","Unpacking swig3.0 (3.0.12-1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n","Unpacking swig (3.0.12-1) ...\n","Setting up swig3.0 (3.0.12-1) ...\n","Setting up swig (3.0.12-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting stable-baselines[mpi]==2.10.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/fe/db8159d4d79109c6c8942abe77c7ba6b6e008c32ae55870a35e73fa10db3/stable_baselines-2.10.0-py3-none-any.whl (248kB)\n","\u001b[K     |████████████████████████████████| 256kB 4.0MB/s \n","\u001b[?25hCollecting box2d\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/1b/ce95bb5d1807d4d85af8d0c90050add1a77124459f8097791f0c39136d53/Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 7.5MB/s \n","\u001b[?25hCollecting box2d-kengz\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/20/51d6c0c87f7642efb709c518fb0ca8e5eab068259588552c41da5926ae27/Box2D-kengz-2.3.3.tar.gz (425kB)\n","\u001b[K     |████████████████████████████████| 430kB 17.3MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n","Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.19.5)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.0.1)\n","Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n","Requirement already satisfied: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n","Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.6)\n","Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines[mpi]==2.10.0) (1.15.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n","Building wheels for collected packages: box2d-kengz\n","  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp37-cp37m-linux_x86_64.whl size=2026011 sha256=916406d83de9cc2bf6e5cf940a84ed9abec610c06c995b567006006aa16938f7\n","  Stored in directory: /root/.cache/pip/wheels/75/ae/e5/8bc678d262caad94659c199c540550e59d03dd3bd3684d4f1a\n","Successfully built box2d-kengz\n","Installing collected packages: stable-baselines, box2d, box2d-kengz\n","  Found existing installation: stable-baselines 2.2.1\n","    Uninstalling stable-baselines-2.2.1:\n","      Successfully uninstalled stable-baselines-2.2.1\n","Successfully installed box2d-2.3.10 box2d-kengz-2.3.3 stable-baselines-2.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iJug4kytSHlH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620451731189,"user_tz":420,"elapsed":13508,"user":{"displayName":"MLAB Team0","photoUrl":"","userId":"09750010280646594005"}},"outputId":"cb2bbe36-b76b-4cc7-9162-b2a30d2fd49e"},"source":["!pip install procgen -q"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 39.9MB 129kB/s \n","\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n","\u001b[K     |████████████████████████████████| 675kB 47.8MB/s \n","\u001b[K     |████████████████████████████████| 22.2MB 1.4MB/s \n","\u001b[K     |████████████████████████████████| 3.3MB 51.4MB/s \n","\u001b[K     |████████████████████████████████| 204kB 56.7MB/s \n","\u001b[K     |████████████████████████████████| 40kB 6.2MB/s \n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OBWDsxKwRYPy","colab":{"base_uri":"https://localhost:8080/","height":171},"executionInfo":{"status":"ok","timestamp":1620451753424,"user_tz":420,"elapsed":6842,"user":{"displayName":"MLAB Team0","photoUrl":"","userId":"09750010280646594005"}},"outputId":"e4e8ec6b-7017-414c-fb9e-00fae43e9ee7"},"source":["import stable_baselines\n","stable_baselines.__version__"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.10.0'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"ImRKtNcIRiGO","executionInfo":{"status":"ok","timestamp":1620451759119,"user_tz":420,"elapsed":711,"user":{"displayName":"MLAB Team0","photoUrl":"","userId":"09750010280646594005"}}},"source":["import os\n","\n","import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from stable_baselines import DDPG, TD3\n","from stable_baselines.ddpg.policies import LnMlpPolicy\n","from stable_baselines.bench import Monitor\n","from stable_baselines.results_plotter import load_results, ts2xy\n","from stable_baselines.common.noise import AdaptiveParamNoiseSpec, NormalActionNoise\n","from stable_baselines.common.callbacks import BaseCallback\n","import time"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"id":"DwibsswFc5yT","executionInfo":{"status":"error","timestamp":1620451820818,"user_tz":420,"elapsed":45567,"user":{"displayName":"MLAB Team0","photoUrl":"","userId":"09750010280646594005"}},"outputId":"e0ccf885-26b1-4a00-bc91-9fbd82dfa24e"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":7,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"x7K1K0AhSTbc","executionInfo":{"status":"ok","timestamp":1620451824163,"user_tz":420,"elapsed":658,"user":{"displayName":"MLAB Team0","photoUrl":"","userId":"09750010280646594005"}}},"source":["log_dir = \"/tmp/gym/\"\n","os.makedirs(log_dir, exist_ok=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"NnKTmnk7RlCn","executionInfo":{"status":"ok","timestamp":1620451826523,"user_tz":420,"elapsed":810,"user":{"displayName":"MLAB Team0","photoUrl":"","userId":"09750010280646594005"}}},"source":["class SaveOnBestTrainingRewardCallback(BaseCallback):\n","    \"\"\"\n","    Callback for saving a model (the check is done every ``check_freq`` steps)\n","    based on the training reward (in practice, we recommend using ``EvalCallback``).\n","\n","    :param check_freq: (int)\n","    :param log_dir: (str) Path to the folder where the model will be saved.\n","      It must contains the file created by the ``Monitor`` wrapper.\n","    :param verbose: (int)\n","    \"\"\"\n","    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n","        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n","        self.check_freq = check_freq\n","        self.log_dir = log_dir\n","        self.save_path = os.path.join(log_dir, 'best_model' )\n","        self.best_mean_reward = -np.inf\n","\n","    def _init_callback(self) -> None:\n","        # Create folder if needed\n","        if self.save_path is not None:\n","            os.makedirs(self.save_path, exist_ok=True)\n","\n","    def _on_step(self) -> bool:\n","        if self.n_calls % self.check_freq == 0:\n","\n","          # Retrieve training reward\n","          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n","          if len(x) > 0:\n","              # Mean training reward over the last 100 episodes\n","              mean_reward = np.mean(y[-100:])\n","              if self.verbose > 0:\n","                print(\"Num timesteps: {}\".format(self.num_timesteps))\n","                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n","\n","              # New best model, you could save the agent here\n","              if mean_reward > self.best_mean_reward:\n","                  self.best_mean_reward = mean_reward\n","                  # Example for saving best model\n","                  if self.verbose > 0:\n","                    print(\"Saving new best model to {}\".format(self.save_path))\n","                  self.model.save(self.save_path)\n","\n","        return True"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"WcZ-U5I4UriF","executionInfo":{"status":"ok","timestamp":1620451829665,"user_tz":420,"elapsed":697,"user":{"displayName":"MLAB Team0","photoUrl":"","userId":"09750010280646594005"}}},"source":["def evaluate(model, num_episodes=100):\n","    \"\"\"\n","    Evaluate a RL agent\n","    :param model: (BaseRLModel object) the RL Agent\n","    :param num_episodes: (int) number of episodes to evaluate it\n","    :return: (float) Mean reward for the last num_episodes\n","    \"\"\"\n","    # This function will only work for a single Environment\n","    env = model.get_env()\n","    all_episode_rewards = []\n","    for i in range(num_episodes):\n","        episode_rewards = []\n","        done = False\n","        obs = env.reset()\n","        while not done:\n","            # _states are only useful when using LSTM policies\n","            action, _states = model.predict(obs)\n","            # here, action, rewards and dones are arrays\n","            # because we are using vectorized env\n","            obs, reward, done, info = env.step(action)\n","            episode_rewards.append(reward)\n","\n","        all_episode_rewards.append(sum(episode_rewards))\n","\n","    mean_episode_reward = np.mean(all_episode_rewards)\n","    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n","\n","    return mean_episode_reward"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRRLhg5cSbBk","executionInfo":{"status":"ok","timestamp":1620451831751,"user_tz":420,"elapsed":647,"user":{"displayName":"MLAB Team0","photoUrl":"","userId":"09750010280646594005"}}},"source":["from stable_baselines.common.policies import MlpPolicy, CnnLstmPolicy\n","from stable_baselines.common.vec_env import DummyVecEnv\n","from stable_baselines import PPO2\n","from stable_baselines import results_plotter\n","from stable_baselines.bench import Monitor\n","from stable_baselines import DQN"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDE9AyvTRUmW","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1620454102721,"user_tz":420,"elapsed":2264598,"user":{"displayName":"MLAB Team0","photoUrl":"","userId":"09750010280646594005"}},"outputId":"5cad86a1-3e4e-4a66-889a-f4aad2177efc"},"source":["param = {\"num_levels\": 50, \"distribution_mode\": \"easy\"}#, \"render_mode\": \"human\"}\n","env = gym.make(\"procgen:procgen-fruitbot-v0\", **param)\n","# log_dir = f\"/tmp/gym/{time.strftime('%d%m%y_%H:%M:%S', time.localtime())}\"\n","callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n","env = Monitor(env, log_dir)\n","kwargs = {'double_q': True, 'prioritized_replay': True}\n","dqn_model = DQN('CnnPolicy', env, verbose=1 , **kwargs)\n","dqn_model.learn(total_timesteps=1000000, log_interval=100, callback=callback)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:322: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","Num timesteps: 1000\n","Best mean reward: -inf - Last mean reward per episode: -2.36\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 2000\n","Best mean reward: -2.36 - Last mean reward per episode: -2.04\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 3000\n","Best mean reward: -2.04 - Last mean reward per episode: -2.58\n","Num timesteps: 4000\n","Best mean reward: -2.04 - Last mean reward per episode: -2.70\n","Num timesteps: 5000\n","Best mean reward: -2.04 - Last mean reward per episode: -2.93\n","Num timesteps: 6000\n","Best mean reward: -2.04 - Last mean reward per episode: -3.05\n","Num timesteps: 7000\n","Best mean reward: -2.04 - Last mean reward per episode: -3.29\n","Num timesteps: 8000\n","Best mean reward: -2.04 - Last mean reward per episode: -3.29\n","--------------------------------------\n","| % time spent exploring  | 91       |\n","| episodes                | 100      |\n","| mean 100 episode reward | -3.3     |\n","| steps                   | 8204     |\n","--------------------------------------\n","Num timesteps: 9000\n","Best mean reward: -2.04 - Last mean reward per episode: -3.12\n","Num timesteps: 10000\n","Best mean reward: -2.04 - Last mean reward per episode: -3.29\n","Num timesteps: 11000\n","Best mean reward: -2.04 - Last mean reward per episode: -3.12\n","Num timesteps: 12000\n","Best mean reward: -2.04 - Last mean reward per episode: -2.90\n","Num timesteps: 13000\n","Best mean reward: -2.04 - Last mean reward per episode: -2.76\n","Num timesteps: 14000\n","Best mean reward: -2.04 - Last mean reward per episode: -2.57\n","Num timesteps: 15000\n","Best mean reward: -2.04 - Last mean reward per episode: -2.01\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 16000\n","Best mean reward: -2.01 - Last mean reward per episode: -1.94\n","Saving new best model to /tmp/gym/best_model\n","--------------------------------------\n","| % time spent exploring  | 84       |\n","| episodes                | 200      |\n","| mean 100 episode reward | -2       |\n","| steps                   | 16304    |\n","--------------------------------------\n","Num timesteps: 17000\n","Best mean reward: -1.94 - Last mean reward per episode: -2.02\n","Num timesteps: 18000\n","Best mean reward: -1.94 - Last mean reward per episode: -1.95\n","Num timesteps: 19000\n","Best mean reward: -1.94 - Last mean reward per episode: -1.95\n","Num timesteps: 20000\n","Best mean reward: -1.94 - Last mean reward per episode: -1.90\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 21000\n","Best mean reward: -1.90 - Last mean reward per episode: -1.83\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 22000\n","Best mean reward: -1.83 - Last mean reward per episode: -2.02\n","Num timesteps: 23000\n","Best mean reward: -1.83 - Last mean reward per episode: -2.03\n","Num timesteps: 24000\n","Best mean reward: -1.83 - Last mean reward per episode: -1.97\n","--------------------------------------\n","| % time spent exploring  | 76       |\n","| episodes                | 300      |\n","| mean 100 episode reward | -1.9     |\n","| steps                   | 24105    |\n","--------------------------------------\n","Num timesteps: 25000\n","Best mean reward: -1.83 - Last mean reward per episode: -2.10\n","Num timesteps: 26000\n","Best mean reward: -1.83 - Last mean reward per episode: -2.18\n","Num timesteps: 27000\n","Best mean reward: -1.83 - Last mean reward per episode: -2.23\n","Num timesteps: 28000\n","Best mean reward: -1.83 - Last mean reward per episode: -2.33\n","Num timesteps: 29000\n","Best mean reward: -1.83 - Last mean reward per episode: -2.19\n","Num timesteps: 30000\n","Best mean reward: -1.83 - Last mean reward per episode: -2.06\n","Num timesteps: 31000\n","Best mean reward: -1.83 - Last mean reward per episode: -2.07\n","Num timesteps: 32000\n","Best mean reward: -1.83 - Last mean reward per episode: -1.94\n","--------------------------------------\n","| % time spent exploring  | 68       |\n","| episodes                | 400      |\n","| mean 100 episode reward | -2       |\n","| steps                   | 32171    |\n","--------------------------------------\n","Num timesteps: 33000\n","Best mean reward: -1.83 - Last mean reward per episode: -1.88\n","Num timesteps: 34000\n","Best mean reward: -1.83 - Last mean reward per episode: -1.93\n","Num timesteps: 35000\n","Best mean reward: -1.83 - Last mean reward per episode: -1.94\n","Num timesteps: 36000\n","Best mean reward: -1.83 - Last mean reward per episode: -1.86\n","Num timesteps: 37000\n","Best mean reward: -1.83 - Last mean reward per episode: -1.89\n","Num timesteps: 38000\n","Best mean reward: -1.83 - Last mean reward per episode: -1.88\n","Num timesteps: 39000\n","Best mean reward: -1.83 - Last mean reward per episode: -1.96\n","--------------------------------------\n","| % time spent exploring  | 60       |\n","| episodes                | 500      |\n","| mean 100 episode reward | -2       |\n","| steps                   | 39928    |\n","--------------------------------------\n","Num timesteps: 40000\n","Best mean reward: -1.83 - Last mean reward per episode: -2.04\n","Num timesteps: 41000\n","Best mean reward: -1.83 - Last mean reward per episode: -1.73\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 42000\n","Best mean reward: -1.73 - Last mean reward per episode: -1.76\n","Num timesteps: 43000\n","Best mean reward: -1.73 - Last mean reward per episode: -1.69\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 44000\n","Best mean reward: -1.69 - Last mean reward per episode: -1.93\n","Num timesteps: 45000\n","Best mean reward: -1.69 - Last mean reward per episode: -1.98\n","Num timesteps: 46000\n","Best mean reward: -1.69 - Last mean reward per episode: -1.95\n","Num timesteps: 47000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.02\n","--------------------------------------\n","| % time spent exploring  | 53       |\n","| episodes                | 600      |\n","| mean 100 episode reward | -1.8     |\n","| steps                   | 47775    |\n","--------------------------------------\n","Num timesteps: 48000\n","Best mean reward: -1.69 - Last mean reward per episode: -1.88\n","Num timesteps: 49000\n","Best mean reward: -1.69 - Last mean reward per episode: -1.81\n","Num timesteps: 50000\n","Best mean reward: -1.69 - Last mean reward per episode: -1.85\n","Num timesteps: 51000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.01\n","Num timesteps: 52000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.03\n","Num timesteps: 53000\n","Best mean reward: -1.69 - Last mean reward per episode: -1.77\n","Num timesteps: 54000\n","Best mean reward: -1.69 - Last mean reward per episode: -1.93\n","Num timesteps: 55000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.08\n","--------------------------------------\n","| % time spent exploring  | 45       |\n","| episodes                | 700      |\n","| mean 100 episode reward | -2.3     |\n","| steps                   | 55867    |\n","--------------------------------------\n","Num timesteps: 56000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.12\n","Num timesteps: 57000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.39\n","Num timesteps: 58000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.44\n","Num timesteps: 59000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.30\n","Num timesteps: 60000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.13\n","Num timesteps: 61000\n","Best mean reward: -1.69 - Last mean reward per episode: -1.99\n","Num timesteps: 62000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.29\n","Num timesteps: 63000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.39\n","Num timesteps: 64000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.44\n","Num timesteps: 65000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.13\n","--------------------------------------\n","| % time spent exploring  | 36       |\n","| episodes                | 800      |\n","| mean 100 episode reward | -2.2     |\n","| steps                   | 65098    |\n","--------------------------------------\n","Num timesteps: 66000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.33\n","Num timesteps: 67000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.12\n","Num timesteps: 68000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.33\n","Num timesteps: 69000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.50\n","Num timesteps: 70000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.46\n","Num timesteps: 71000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.42\n","Num timesteps: 72000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.56\n","Num timesteps: 73000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.60\n","--------------------------------------\n","| % time spent exploring  | 28       |\n","| episodes                | 900      |\n","| mean 100 episode reward | -2.7     |\n","| steps                   | 73293    |\n","--------------------------------------\n","Num timesteps: 74000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.44\n","Num timesteps: 75000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.68\n","Num timesteps: 76000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.46\n","Num timesteps: 77000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.32\n","Num timesteps: 78000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.22\n","Num timesteps: 79000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.56\n","Num timesteps: 80000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.45\n","Num timesteps: 81000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.31\n","Num timesteps: 82000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.27\n","--------------------------------------\n","| % time spent exploring  | 19       |\n","| episodes                | 1000     |\n","| mean 100 episode reward | -2.1     |\n","| steps                   | 82071    |\n","--------------------------------------\n","Num timesteps: 83000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.24\n","Num timesteps: 84000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.10\n","Num timesteps: 85000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.43\n","Num timesteps: 86000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.31\n","Num timesteps: 87000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.36\n","Num timesteps: 88000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.10\n","Num timesteps: 89000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.29\n","Num timesteps: 90000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.50\n","--------------------------------------\n","| % time spent exploring  | 10       |\n","| episodes                | 1100     |\n","| mean 100 episode reward | -2.5     |\n","| steps                   | 90943    |\n","--------------------------------------\n","Num timesteps: 91000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.56\n","Num timesteps: 92000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.48\n","Num timesteps: 93000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.41\n","Num timesteps: 94000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.32\n","Num timesteps: 95000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.09\n","Num timesteps: 96000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.30\n","Num timesteps: 97000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.28\n","Num timesteps: 98000\n","Best mean reward: -1.69 - Last mean reward per episode: -2.03\n","Num timesteps: 99000\n","Best mean reward: -1.69 - Last mean reward per episode: -1.81\n","Num timesteps: 100000\n","Best mean reward: -1.69 - Last mean reward per episode: -1.64\n","Saving new best model to /tmp/gym/best_model\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 1200     |\n","| mean 100 episode reward | -1.6     |\n","| steps                   | 100019   |\n","--------------------------------------\n","Num timesteps: 101000\n","Best mean reward: -1.64 - Last mean reward per episode: -1.99\n","Num timesteps: 102000\n","Best mean reward: -1.64 - Last mean reward per episode: -1.89\n","Num timesteps: 103000\n","Best mean reward: -1.64 - Last mean reward per episode: -2.14\n","Num timesteps: 104000\n","Best mean reward: -1.64 - Last mean reward per episode: -1.97\n","Num timesteps: 105000\n","Best mean reward: -1.64 - Last mean reward per episode: -1.85\n","Num timesteps: 106000\n","Best mean reward: -1.64 - Last mean reward per episode: -1.83\n","Num timesteps: 107000\n","Best mean reward: -1.64 - Last mean reward per episode: -1.77\n","Num timesteps: 108000\n","Best mean reward: -1.64 - Last mean reward per episode: -1.64\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 1300     |\n","| mean 100 episode reward | -1.6     |\n","| steps                   | 108980   |\n","--------------------------------------\n","Num timesteps: 109000\n","Best mean reward: -1.64 - Last mean reward per episode: -1.62\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 110000\n","Best mean reward: -1.62 - Last mean reward per episode: -1.27\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 111000\n","Best mean reward: -1.27 - Last mean reward per episode: -1.22\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 112000\n","Best mean reward: -1.22 - Last mean reward per episode: -1.16\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 113000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.35\n","Num timesteps: 114000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.47\n","Num timesteps: 115000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.84\n","Num timesteps: 116000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.86\n","Num timesteps: 117000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.06\n","Num timesteps: 118000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.15\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 1400     |\n","| mean 100 episode reward | -2.2     |\n","| steps                   | 118193   |\n","--------------------------------------\n","Num timesteps: 119000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.27\n","Num timesteps: 120000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.68\n","Num timesteps: 121000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.48\n","Num timesteps: 122000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.38\n","Num timesteps: 123000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.55\n","Num timesteps: 124000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.16\n","Num timesteps: 125000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.23\n","Num timesteps: 126000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.49\n","Num timesteps: 127000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.64\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 1500     |\n","| mean 100 episode reward | -2.7     |\n","| steps                   | 127407   |\n","--------------------------------------\n","Num timesteps: 128000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.44\n","Num timesteps: 129000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.71\n","Num timesteps: 130000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.85\n","Num timesteps: 131000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.87\n","Num timesteps: 132000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.54\n","Num timesteps: 133000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.45\n","Num timesteps: 134000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.48\n","Num timesteps: 135000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.17\n","Num timesteps: 136000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.73\n","Num timesteps: 137000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.84\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 1600     |\n","| mean 100 episode reward | -1.8     |\n","| steps                   | 137031   |\n","--------------------------------------\n","Num timesteps: 138000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.93\n","Num timesteps: 139000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.48\n","Num timesteps: 140000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.20\n","Num timesteps: 141000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.20\n","Num timesteps: 142000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.39\n","Num timesteps: 143000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.29\n","Num timesteps: 144000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.37\n","Num timesteps: 145000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.59\n","Num timesteps: 146000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.77\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 1700     |\n","| mean 100 episode reward | -2       |\n","| steps                   | 146836   |\n","--------------------------------------\n","Num timesteps: 147000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.95\n","Num timesteps: 148000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.95\n","Num timesteps: 149000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.00\n","Num timesteps: 150000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.22\n","Num timesteps: 151000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.36\n","Num timesteps: 152000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.55\n","Num timesteps: 153000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.43\n","Num timesteps: 154000\n","Best mean reward: -1.16 - Last mean reward per episode: -2.38\n","Num timesteps: 155000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.92\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 1800     |\n","| mean 100 episode reward | -1.7     |\n","| steps                   | 155914   |\n","--------------------------------------\n","Num timesteps: 156000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.67\n","Num timesteps: 157000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.71\n","Num timesteps: 158000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.43\n","Num timesteps: 159000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.25\n","Num timesteps: 160000\n","Best mean reward: -1.16 - Last mean reward per episode: -1.16\n","Num timesteps: 161000\n","Best mean reward: -1.16 - Last mean reward per episode: -0.96\n","Saving new best model to /tmp/gym/best_model\n","Num timesteps: 162000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.10\n","Num timesteps: 163000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.39\n","Num timesteps: 164000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.52\n","Num timesteps: 165000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.76\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 1900     |\n","| mean 100 episode reward | -1.6     |\n","| steps                   | 165562   |\n","--------------------------------------\n","Num timesteps: 166000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.79\n","Num timesteps: 167000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.00\n","Num timesteps: 168000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.14\n","Num timesteps: 169000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.14\n","Num timesteps: 170000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.35\n","Num timesteps: 171000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.26\n","Num timesteps: 172000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.13\n","Num timesteps: 173000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.25\n","Num timesteps: 174000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.15\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 2000     |\n","| mean 100 episode reward | -2.2     |\n","| steps                   | 174774   |\n","--------------------------------------\n","Num timesteps: 175000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.20\n","Num timesteps: 176000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.32\n","Num timesteps: 177000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.26\n","Num timesteps: 178000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.12\n","Num timesteps: 179000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.12\n","Num timesteps: 180000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.16\n","Num timesteps: 181000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.99\n","Num timesteps: 182000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.73\n","Num timesteps: 183000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.77\n","Num timesteps: 184000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.90\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 2100     |\n","| mean 100 episode reward | -2       |\n","| steps                   | 184435   |\n","--------------------------------------\n","Num timesteps: 185000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.93\n","Num timesteps: 186000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.62\n","Num timesteps: 187000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.85\n","Num timesteps: 188000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.36\n","Num timesteps: 189000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.58\n","Num timesteps: 190000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.59\n","Num timesteps: 191000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.67\n","Num timesteps: 192000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.88\n","Num timesteps: 193000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.88\n","Num timesteps: 194000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.64\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 2200     |\n","| mean 100 episode reward | -2.6     |\n","| steps                   | 194183   |\n","--------------------------------------\n","Num timesteps: 195000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.73\n","Num timesteps: 196000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.98\n","Num timesteps: 197000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.73\n","Num timesteps: 198000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.37\n","Num timesteps: 199000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.90\n","Num timesteps: 200000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.14\n","Num timesteps: 201000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.10\n","Num timesteps: 202000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.26\n","Num timesteps: 203000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.12\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 2300     |\n","| mean 100 episode reward | -2.1     |\n","| steps                   | 203638   |\n","--------------------------------------\n","Num timesteps: 204000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.04\n","Num timesteps: 205000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.84\n","Num timesteps: 206000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.72\n","Num timesteps: 207000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.54\n","Num timesteps: 208000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.75\n","Num timesteps: 209000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.59\n","Num timesteps: 210000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.80\n","Num timesteps: 211000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.62\n","Num timesteps: 212000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.61\n","Num timesteps: 213000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.84\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 2400     |\n","| mean 100 episode reward | -1.8     |\n","| steps                   | 213119   |\n","--------------------------------------\n","Num timesteps: 214000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.79\n","Num timesteps: 215000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.90\n","Num timesteps: 216000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.93\n","Num timesteps: 217000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.98\n","Num timesteps: 218000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.83\n","Num timesteps: 219000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.76\n","Num timesteps: 220000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.84\n","Num timesteps: 221000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.87\n","Num timesteps: 222000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.61\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 2500     |\n","| mean 100 episode reward | -1.7     |\n","| steps                   | 222331   |\n","--------------------------------------\n","Num timesteps: 223000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.70\n","Num timesteps: 224000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.87\n","Num timesteps: 225000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.75\n","Num timesteps: 226000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.94\n","Num timesteps: 227000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.11\n","Num timesteps: 228000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.20\n","Num timesteps: 229000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.39\n","Num timesteps: 230000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.51\n","Num timesteps: 231000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.47\n","Num timesteps: 232000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.68\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 2600     |\n","| mean 100 episode reward | -2.7     |\n","| steps                   | 232390   |\n","--------------------------------------\n","Num timesteps: 233000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.65\n","Num timesteps: 234000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.66\n","Num timesteps: 235000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.66\n","Num timesteps: 236000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.49\n","Num timesteps: 237000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.28\n","Num timesteps: 238000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.21\n","Num timesteps: 239000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.97\n","Num timesteps: 240000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.78\n","Num timesteps: 241000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.75\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 2700     |\n","| mean 100 episode reward | -1.8     |\n","| steps                   | 241635   |\n","--------------------------------------\n","Num timesteps: 242000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.93\n","Num timesteps: 243000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.90\n","Num timesteps: 244000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.88\n","Num timesteps: 245000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.15\n","Num timesteps: 246000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.33\n","Num timesteps: 247000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.34\n","Num timesteps: 248000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.49\n","Num timesteps: 249000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.37\n","Num timesteps: 250000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.21\n","Num timesteps: 251000\n","Best mean reward: -0.96 - Last mean reward per episode: -2.11\n","--------------------------------------\n","| % time spent exploring  | 2        |\n","| episodes                | 2800     |\n","| mean 100 episode reward | -2.1     |\n","| steps                   | 251249   |\n","--------------------------------------\n","Num timesteps: 252000\n","Best mean reward: -0.96 - Last mean reward per episode: -1.91\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-378abec5e640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'double_q'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prioritized_replay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdqn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CnnPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdqn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, replay_wrapper)\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                         _, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1, dones, weights,\n\u001b[0;32m--> 295\u001b[0;31m                                                         sess=self.sess)\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprioritized_replay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sess, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"rAVQ5rwySFVK"},"source":["mean_reward = evaluate(dqn_model, num_episodes=100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QABONVqTUmS2"},"source":[""],"execution_count":null,"outputs":[]}]}